{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dd5d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4842b9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import syllables\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords as nltk_sw\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39309c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"Input.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22102dea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blackassign0001</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blackassign0002</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blackassign0003</td>\n",
       "      <td>https://insights.blackcoffer.com/internet-dema...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blackassign0004</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-cyber...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blackassign0005</td>\n",
       "      <td>https://insights.blackcoffer.com/ott-platform-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>blackassign0096</td>\n",
       "      <td>https://insights.blackcoffer.com/what-is-the-r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>blackassign0097</td>\n",
       "      <td>https://insights.blackcoffer.com/impact-of-cov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>blackassign0098</td>\n",
       "      <td>https://insights.blackcoffer.com/contribution-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>blackassign0099</td>\n",
       "      <td>https://insights.blackcoffer.com/how-covid-19-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>blackassign0100</td>\n",
       "      <td>https://insights.blackcoffer.com/how-will-covi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             URL_ID                                                URL\n",
       "0   blackassign0001  https://insights.blackcoffer.com/rising-it-cit...\n",
       "1   blackassign0002  https://insights.blackcoffer.com/rising-it-cit...\n",
       "2   blackassign0003  https://insights.blackcoffer.com/internet-dema...\n",
       "3   blackassign0004  https://insights.blackcoffer.com/rise-of-cyber...\n",
       "4   blackassign0005  https://insights.blackcoffer.com/ott-platform-...\n",
       "..              ...                                                ...\n",
       "95  blackassign0096  https://insights.blackcoffer.com/what-is-the-r...\n",
       "96  blackassign0097  https://insights.blackcoffer.com/impact-of-cov...\n",
       "97  blackassign0098  https://insights.blackcoffer.com/contribution-...\n",
       "98  blackassign0099  https://insights.blackcoffer.com/how-covid-19-...\n",
       "99  blackassign0100  https://insights.blackcoffer.com/how-will-covi...\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d1d5563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['https://insights.blackcoffer.com/rising-it-cities-and-its-impact-on-the-economy-environment-infrastructure-and-city-life-by-the-year-2040-2/',\n",
       "  'https://insights.blackcoffer.com/rising-it-cities-and-their-impact-on-the-economy-environment-infrastructure-and-city-life-in-future/',\n",
       "  'https://insights.blackcoffer.com/internet-demands-evolution-communication-impact-and-2035s-alternative-pathways/',\n",
       "  'https://insights.blackcoffer.com/rise-of-cybercrime-and-its-effect-in-upcoming-future/',\n",
       "  'https://insights.blackcoffer.com/ott-platform-and-its-impact-on-the-entertainment-industry-in-future/',\n",
       "  'https://insights.blackcoffer.com/the-rise-of-the-ott-platform-and-its-impact-on-the-entertainment-industry-by-2040/',\n",
       "  'https://insights.blackcoffer.com/rise-of-cyber-crime-and-its-effects/',\n",
       "  'https://insights.blackcoffer.com/rise-of-internet-demand-and-its-impact-on-communications-and-alternatives-by-the-year-2035-2/',\n",
       "  'https://insights.blackcoffer.com/rise-of-cybercrime-and-its-effect-by-the-year-2040-2/',\n",
       "  'https://insights.blackcoffer.com/rise-of-cybercrime-and-its-effect-by-the-year-2040/'],\n",
       " 100)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls = list(df[\"URL\"])\n",
    "urls[:10], len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e44bb65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rise of Internet Demand and Its Impact on Communications and Alternatives by the Year 2035\\nIntroduction:\\nThe year 2035 is anticipated to witness an unprecedented surge in internet demand, catalysed by rapid technological advancements and the proliferation of connected devices. This growth in internet consumption will have far-reaching implications for global communications and will reshape the way individuals, businesses, and societies interact with each other. In this article, we explore Intern'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fetch_web_data(url):\n",
    "    class_ = [\"td-post-content tagdiv-type\", \"tdb-block-inner td-fix-index\"]\n",
    "    doc = requests.get(url)\n",
    "    soup = BeautifulSoup(doc.content, \"html.parser\")\n",
    "    title = soup.find(\"h1\")\n",
    "    article = soup.find_all(\"div\", {\"class\": class_[0]})\n",
    "    if article:\n",
    "        res = \" \"\n",
    "        for tag in article:\n",
    "            res += tag.text.strip()\n",
    "    else:\n",
    "        article = soup.find_all(\"div\", {\"class\": class_[1]})\n",
    "        res = \" \"\n",
    "        for tag in article:\n",
    "            res += tag.text.strip()\n",
    "    try:\n",
    "        start = res.index(\"Introduction\")\n",
    "        stop = res.index(\"Blackcoffer Insights\")\n",
    "    except:\n",
    "        start = 0\n",
    "        stop = -1\n",
    "    return title.text + \"\\n\" + res[start:stop]\n",
    "\n",
    "\n",
    "fetch_web_data(random.choice(urls))[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddc64d75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ERNST',\n",
       " 'YOUNG',\n",
       " 'DELOITTE',\n",
       " 'TOUCHE',\n",
       " 'KPMG',\n",
       " 'PRICEWATERHOUSECOOPERS',\n",
       " 'PRICEWATERHOUSE',\n",
       " 'COOPERS',\n",
       " 'AFGHANI',\n",
       " 'ARIARY']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_stop_words():\n",
    "    StopWords_notNames = []\n",
    "    for file in os.listdir(\"StopWords/\"):\n",
    "        if file != \"StopWords_Names.txt\":\n",
    "            corpus = open(f\"StopWords/{file}\", \"r\").read().strip(\" \").split(\"\\n\")\n",
    "            res = []\n",
    "            for txt in corpus:\n",
    "                if \"|\" in txt:\n",
    "                    res.extend(txt.replace(\" | \", \",\").replace(\" \", \"\").split(\",\"))\n",
    "            if res != []:\n",
    "                StopWords_notNames.extend(res)\n",
    "\n",
    "    StopWords_Names = []\n",
    "    for file in os.listdir(\"StopWords/\"):\n",
    "        if file == \"StopWords_Names.txt\":\n",
    "            corpus = open(f\"StopWords/{file}\", \"r\").read().strip(\" \").split(\"\\n\")\n",
    "            for txt in corpus:\n",
    "                if \"|\" in txt:\n",
    "                    res = txt.replace(\" | \", \",\").replace(\" \", \"\").split(\",\")\n",
    "                    if res != None:\n",
    "                        StopWords_Names.append(res[0])\n",
    "\n",
    "    stop_words = []\n",
    "    for file in os.listdir(\"StopWords/\"):\n",
    "        corpus = open(f\"StopWords/{file}\", \"r\").read().strip().split(\"\\n\")\n",
    "        res = []\n",
    "        for txt in corpus:\n",
    "            if \"|\" in txt:\n",
    "                txt = txt.replace(txt, txt.split(\"|\")[0])\n",
    "                res.append(txt.strip())\n",
    "        if res != []:\n",
    "            stop_words.extend(res)\n",
    "        stop_words.extend([txt for txt in corpus if \"|\" not in txt])\n",
    "\n",
    "    stop_words.extend(StopWords_notNames)\n",
    "    stop_words.extend(StopWords_Names)\n",
    "    return stop_words\n",
    "\n",
    "\n",
    "get_stop_words()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eaaad66a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\91636\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\91636\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\91636\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\91636\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\91636\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\91636\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18066355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords as nltk_sw\n",
    "import re\n",
    "\n",
    "def get_stop_words():\n",
    "    # Define your own stop words here if you have any\n",
    "    personal_stop_words = ['exampleword1', 'exampleword2']\n",
    "    return personal_stop_words\n",
    "\n",
    "def clean_stop_words(text, personalwords=True):\n",
    "    stop_words = get_stop_words()\n",
    "    if personalwords:\n",
    "        try:\n",
    "            stop_words.extend(nltk_sw.words(\"english\"))\n",
    "        except LookupError:\n",
    "            nltk.download('stopwords')\n",
    "            stop_words.extend(nltk_sw.words(\"english\"))\n",
    "    \n",
    "    words = text.split()\n",
    "    cleaned_words = [word for word in words if word.lower() not in stop_words]\n",
    "    cleaned_text = \" \".join(cleaned_words)\n",
    "    cleaned_text = \" \".join(re.findall(\"[a-zA-Z.]+\", cleaned_text))\n",
    "    return cleaned_text\n",
    "\n",
    "# Assuming fetch_web_data and urls are defined elsewhere in your code\n",
    "# text = fetch_web_data(random.choice(urls))\n",
    "# print(clean_stop_words(text)[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b15e091",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\91636\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Rise electric vehicle impact livelihood year . Additionally electrification forms road transportation continues increase. Around two wheelers road around globe electric however trend mostly restricted China India ASEAN nations. world s worst air quality India. Even Covid crisis national state lockdowns cities bad air quality India. Internal combustion engines ICE particular greatly worsened vehicular pollution result growing number private automobiles Indian roads. India third highest oil consum'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_stop_words(fetch_web_data(random.choice(urls)))[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7ff2002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34, 76, -0.3783783783783784, 0.7422003284072249)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_scores(text):\n",
    "    def get_subjectivity_score(text):\n",
    "        num_words = len(text.split())\n",
    "        unique_words = len(set(text.split()))\n",
    "        subjectivity_score = unique_words / num_words\n",
    "        return subjectivity_score\n",
    "\n",
    "    def get_polarity_score(text):\n",
    "        positive_words = (\n",
    "            open(\"MasterDictionary/positive-words.txt\", \"r\").read().split(\"\\n\")\n",
    "        )\n",
    "        negative_words = (\n",
    "            open(\"MasterDictionary/negative-words.txt\", \"r\").read().split(\"\\n\")\n",
    "        )\n",
    "\n",
    "        positive_count = 0\n",
    "        negative_count = 0\n",
    "\n",
    "        for word in text.split():\n",
    "            if word.lower() in positive_words:\n",
    "                positive_count += 1\n",
    "            elif word.lower() in negative_words:\n",
    "                negative_count += 1\n",
    "\n",
    "        polarity_score = (positive_count - negative_count) / (\n",
    "            positive_count + negative_count + 1\n",
    "        )\n",
    "        return polarity_score, positive_count, negative_count\n",
    "\n",
    "    subjectivity_score = get_subjectivity_score(text)\n",
    "    polarity_score, positive_count, negative_count = get_polarity_score(text)\n",
    "\n",
    "    return positive_count, negative_count, polarity_score, subjectivity_score\n",
    "\n",
    "\n",
    "get_scores(clean_stop_words(fetch_web_data(random.choice(urls))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c1ed18f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51,\n",
       " 12.660377358490566,\n",
       " 0.07600596125186289,\n",
       " 5.094553327896971,\n",
       " 12.660377358490566,\n",
       " 2.323397913561848)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Analysis_of_readability(fetched_article):\n",
    "    sentences = fetched_article.replace(\" \", \"\").split(\".\")\n",
    "    tokens = fetched_article.split(\" \")\n",
    "    total_num_of_sentences = len(sentences)\n",
    "    total_num_of_words = len(tokens)\n",
    "\n",
    "    num_complex_words = 0\n",
    "    for token in sentences:\n",
    "        if syllables.estimate(token) > 2:\n",
    "            num_complex_words += 1\n",
    "\n",
    "    Average_Sentence_Length = total_num_of_words / total_num_of_sentences\n",
    "    Percentage_of_Complex_words = num_complex_words / total_num_of_words\n",
    "    Fog_Index = 0.4 * (Average_Sentence_Length + Percentage_of_Complex_words)\n",
    "\n",
    "    Average_Number_of_Words_Per_Sentence = total_num_of_words / total_num_of_sentences\n",
    "\n",
    "    total_syllables = sum(syllables.estimate(word) for word in sentences)\n",
    "    SYLLABLE_PER_WORD = total_syllables / total_num_of_words\n",
    "    SYLLABLE_PER_WORD\n",
    "\n",
    "    return (\n",
    "        num_complex_words,\n",
    "        Average_Sentence_Length,\n",
    "        Percentage_of_Complex_words,\n",
    "        Fog_Index,\n",
    "        Average_Number_of_Words_Per_Sentence,\n",
    "        SYLLABLE_PER_WORD,\n",
    "    )\n",
    "\n",
    "\n",
    "Analysis_of_readability(clean_stop_words(fetch_web_data(random.choice(urls))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4837042e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58, 4.578160059835453)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_personal_pronouns(tokens):\n",
    "    personal_pronouns = [\n",
    "        \"I\",\n",
    "        \"me\",\n",
    "        \"my\",\n",
    "        \"mine\",\n",
    "        \"you\",\n",
    "        \"your\",\n",
    "        \"yours\",\n",
    "        \"he\",\n",
    "        \"him\",\n",
    "        \"his\",\n",
    "        \"she\",\n",
    "        \"her\",\n",
    "        \"hers\",\n",
    "        \"it\",\n",
    "        \"its\",\n",
    "        \"we\",\n",
    "        \"us\",\n",
    "        \"our\",\n",
    "        \"ours\",\n",
    "        \"they\",\n",
    "        \"them\",\n",
    "        \"their\",\n",
    "        \"theirs\",\n",
    "    ]\n",
    "    num_personal_pronouns = sum(\n",
    "        [1 for word in tokens if word.lower() in personal_pronouns]\n",
    "    )\n",
    "\n",
    "    total_chars = sum(len(word) for word in tokens)\n",
    "    avg_word_length = total_chars / len(tokens)\n",
    "\n",
    "    return num_personal_pronouns, avg_word_length\n",
    "\n",
    "\n",
    "corpus = clean_stop_words(fetch_web_data(random.choice(urls)),personalwords=False)\n",
    "\n",
    "res = re.findall(\"[A-Za-z]+\", corpus)\n",
    "get_personal_pronouns(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09a2c296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page https://insights.blackcoffer.com/an-outlook-of-healthcare-by-the-year-2040-and-how-it-will-impact-human-lives/ Not Found....!\n",
      "Page https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/ Not Found....!\n",
      "Page https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/ Not Found....!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>URL_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>blackassign0001</th>\n",
       "      <td>42</td>\n",
       "      <td>6</td>\n",
       "      <td>0.734694</td>\n",
       "      <td>0.499244</td>\n",
       "      <td>15.150000</td>\n",
       "      <td>0.065182</td>\n",
       "      <td>6.086073</td>\n",
       "      <td>15.150000</td>\n",
       "      <td>79</td>\n",
       "      <td>4809</td>\n",
       "      <td>1.599010</td>\n",
       "      <td>69</td>\n",
       "      <td>4.591653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blackassign0002</th>\n",
       "      <td>64</td>\n",
       "      <td>25</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.656319</td>\n",
       "      <td>17.792683</td>\n",
       "      <td>0.056203</td>\n",
       "      <td>7.139554</td>\n",
       "      <td>17.792683</td>\n",
       "      <td>82</td>\n",
       "      <td>7342</td>\n",
       "      <td>1.904044</td>\n",
       "      <td>34</td>\n",
       "      <td>5.400404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blackassign0003</th>\n",
       "      <td>40</td>\n",
       "      <td>22</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.592163</td>\n",
       "      <td>18.368421</td>\n",
       "      <td>0.053486</td>\n",
       "      <td>7.368763</td>\n",
       "      <td>18.368421</td>\n",
       "      <td>56</td>\n",
       "      <td>6131</td>\n",
       "      <td>2.154728</td>\n",
       "      <td>28</td>\n",
       "      <td>5.997227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blackassign0004</th>\n",
       "      <td>37</td>\n",
       "      <td>70</td>\n",
       "      <td>-0.305556</td>\n",
       "      <td>0.701016</td>\n",
       "      <td>19.961538</td>\n",
       "      <td>0.050096</td>\n",
       "      <td>8.004654</td>\n",
       "      <td>19.961538</td>\n",
       "      <td>52</td>\n",
       "      <td>5983</td>\n",
       "      <td>2.089595</td>\n",
       "      <td>20</td>\n",
       "      <td>5.911571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blackassign0005</th>\n",
       "      <td>25</td>\n",
       "      <td>8</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.736239</td>\n",
       "      <td>16.536585</td>\n",
       "      <td>0.060472</td>\n",
       "      <td>6.638823</td>\n",
       "      <td>16.536585</td>\n",
       "      <td>41</td>\n",
       "      <td>3510</td>\n",
       "      <td>1.840708</td>\n",
       "      <td>20</td>\n",
       "      <td>5.400850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blackassign0096</th>\n",
       "      <td>30</td>\n",
       "      <td>51</td>\n",
       "      <td>-0.256098</td>\n",
       "      <td>0.706408</td>\n",
       "      <td>21.150943</td>\n",
       "      <td>0.045495</td>\n",
       "      <td>8.478575</td>\n",
       "      <td>21.150943</td>\n",
       "      <td>51</td>\n",
       "      <td>5301</td>\n",
       "      <td>1.796610</td>\n",
       "      <td>6</td>\n",
       "      <td>5.166813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blackassign0097</th>\n",
       "      <td>30</td>\n",
       "      <td>33</td>\n",
       "      <td>-0.046875</td>\n",
       "      <td>0.620996</td>\n",
       "      <td>26.875000</td>\n",
       "      <td>0.036279</td>\n",
       "      <td>10.764512</td>\n",
       "      <td>26.875000</td>\n",
       "      <td>39</td>\n",
       "      <td>4118</td>\n",
       "      <td>1.560000</td>\n",
       "      <td>45</td>\n",
       "      <td>4.661202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blackassign0098</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.738462</td>\n",
       "      <td>15.480000</td>\n",
       "      <td>0.056848</td>\n",
       "      <td>6.214739</td>\n",
       "      <td>15.480000</td>\n",
       "      <td>22</td>\n",
       "      <td>2094</td>\n",
       "      <td>1.839793</td>\n",
       "      <td>7</td>\n",
       "      <td>5.446115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blackassign0099</th>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.698210</td>\n",
       "      <td>18.285714</td>\n",
       "      <td>0.054688</td>\n",
       "      <td>7.336161</td>\n",
       "      <td>18.285714</td>\n",
       "      <td>35</td>\n",
       "      <td>2844</td>\n",
       "      <td>1.681250</td>\n",
       "      <td>15</td>\n",
       "      <td>4.868421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blackassign0100</th>\n",
       "      <td>41</td>\n",
       "      <td>56</td>\n",
       "      <td>-0.153061</td>\n",
       "      <td>0.734756</td>\n",
       "      <td>30.771429</td>\n",
       "      <td>0.032498</td>\n",
       "      <td>12.321571</td>\n",
       "      <td>30.771429</td>\n",
       "      <td>35</td>\n",
       "      <td>5141</td>\n",
       "      <td>1.816156</td>\n",
       "      <td>15</td>\n",
       "      <td>5.211903</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>97 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 POSITIVE SCORE  NEGATIVE SCORE  POLARITY SCORE  \\\n",
       "URL_ID                                                            \n",
       "blackassign0001              42               6        0.734694   \n",
       "blackassign0002              64              25        0.433333   \n",
       "blackassign0003              40              22        0.285714   \n",
       "blackassign0004              37              70       -0.305556   \n",
       "blackassign0005              25               8        0.500000   \n",
       "...                         ...             ...             ...   \n",
       "blackassign0096              30              51       -0.256098   \n",
       "blackassign0097              30              33       -0.046875   \n",
       "blackassign0098               6               3        0.300000   \n",
       "blackassign0099              24               2        0.814815   \n",
       "blackassign0100              41              56       -0.153061   \n",
       "\n",
       "                 SUBJECTIVITY SCORE  AVG SENTENCE LENGTH  \\\n",
       "URL_ID                                                     \n",
       "blackassign0001            0.499244            15.150000   \n",
       "blackassign0002            0.656319            17.792683   \n",
       "blackassign0003            0.592163            18.368421   \n",
       "blackassign0004            0.701016            19.961538   \n",
       "blackassign0005            0.736239            16.536585   \n",
       "...                             ...                  ...   \n",
       "blackassign0096            0.706408            21.150943   \n",
       "blackassign0097            0.620996            26.875000   \n",
       "blackassign0098            0.738462            15.480000   \n",
       "blackassign0099            0.698210            18.285714   \n",
       "blackassign0100            0.734756            30.771429   \n",
       "\n",
       "                 PERCENTAGE OF COMPLEX WORDS  FOG INDEX  \\\n",
       "URL_ID                                                    \n",
       "blackassign0001                     0.065182   6.086073   \n",
       "blackassign0002                     0.056203   7.139554   \n",
       "blackassign0003                     0.053486   7.368763   \n",
       "blackassign0004                     0.050096   8.004654   \n",
       "blackassign0005                     0.060472   6.638823   \n",
       "...                                      ...        ...   \n",
       "blackassign0096                     0.045495   8.478575   \n",
       "blackassign0097                     0.036279  10.764512   \n",
       "blackassign0098                     0.056848   6.214739   \n",
       "blackassign0099                     0.054688   7.336161   \n",
       "blackassign0100                     0.032498  12.321571   \n",
       "\n",
       "                 AVG NUMBER OF WORDS PER SENTENCE   COMPLEX WORD COUNT  \\\n",
       "URL_ID                                                                   \n",
       "blackassign0001                         15.150000                   79   \n",
       "blackassign0002                         17.792683                   82   \n",
       "blackassign0003                         18.368421                   56   \n",
       "blackassign0004                         19.961538                   52   \n",
       "blackassign0005                         16.536585                   41   \n",
       "...                                           ...                  ...   \n",
       "blackassign0096                         21.150943                   51   \n",
       "blackassign0097                         26.875000                   39   \n",
       "blackassign0098                         15.480000                   22   \n",
       "blackassign0099                         18.285714                   35   \n",
       "blackassign0100                         30.771429                   35   \n",
       "\n",
       "                 WORD COUNT   SYLLABLE PER WORD   PERSONAL PRONOUNS  \\\n",
       "URL_ID                                                                \n",
       "blackassign0001         4809            1.599010                 69   \n",
       "blackassign0002         7342            1.904044                 34   \n",
       "blackassign0003         6131            2.154728                 28   \n",
       "blackassign0004         5983            2.089595                 20   \n",
       "blackassign0005         3510            1.840708                 20   \n",
       "...                      ...                 ...                ...   \n",
       "blackassign0096         5301            1.796610                  6   \n",
       "blackassign0097         4118            1.560000                 45   \n",
       "blackassign0098         2094            1.839793                  7   \n",
       "blackassign0099         2844            1.681250                 15   \n",
       "blackassign0100         5141            1.816156                 15   \n",
       "\n",
       "                 AVG WORD LENGTH  \n",
       "URL_ID                            \n",
       "blackassign0001         4.591653  \n",
       "blackassign0002         5.400404  \n",
       "blackassign0003         5.997227  \n",
       "blackassign0004         5.911571  \n",
       "blackassign0005         5.400850  \n",
       "...                          ...  \n",
       "blackassign0096         5.166813  \n",
       "blackassign0097         4.661202  \n",
       "blackassign0098         5.446115  \n",
       "blackassign0099         4.868421  \n",
       "blackassign0100         5.211903  \n",
       "\n",
       "[97 rows x 13 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_r = []\n",
    "url_r = []\n",
    "pos_score_r = []\n",
    "neg_score_r = []\n",
    "Polarity_Score_r = []\n",
    "Polarity_Score_r = []\n",
    "Subjectivity_Score_r = []\n",
    "Average_Sentence_Length_r = []\n",
    "Percentage_of_Complex_words_r = []\n",
    "Fog_Index_r = []\n",
    "Average_Number_of_Words_Per_Sentence_r = []\n",
    "num_complex_words_r = []\n",
    "total_num_of_words_r = []\n",
    "SYLLABLE_PER_WORD_r = []\n",
    "num_personal_pronouns_r = []\n",
    "avg_word_length_r = []\n",
    "\n",
    "\n",
    "# Iterating through URLS\n",
    "for n in range(len(urls)):\n",
    "    try:\n",
    "        # fetch_web_data\n",
    "        fetched_article = fetch_web_data(urls[n])\n",
    "    except:\n",
    "        print(f\"Page {urls[n]} Not Found....!\")\n",
    "        continue\n",
    "    index = df.iloc[n]\n",
    "    id_ = index[0]\n",
    "    url_ = index[1]\n",
    "\n",
    "    # clean_stop_words\n",
    "    tokens = clean_stop_words(fetched_article)\n",
    "    total_num_of_words = len(tokens)\n",
    "\n",
    "    pos_score, neg_score, Polarity_Score, Subjectivity_Score = get_scores(tokens)\n",
    "\n",
    "    (\n",
    "        num_complex_words,\n",
    "        Average_Sentence_Length,\n",
    "        Percentage_of_Complex_words,\n",
    "        Fog_Index,\n",
    "        Average_Number_of_Words_Per_Sentence,\n",
    "        SYLLABLE_PER_WORD,\n",
    "    ) = Analysis_of_readability(fetched_article)\n",
    "\n",
    "    tmp=clean_stop_words(fetched_article,personalwords=False)\n",
    "    res = re.findall(\"[A-Za-z]+\", tmp)\n",
    "    num_personal_pronouns, avg_word_length = get_personal_pronouns(res)\n",
    "\n",
    "    # Appending obtained variables into respective lists\n",
    "    id_r.append(id_)\n",
    "    url_r.append(url_)\n",
    "    pos_score_r.append(pos_score)\n",
    "    neg_score_r.append(neg_score)\n",
    "    Polarity_Score_r.append(Polarity_Score)\n",
    "    Subjectivity_Score_r.append(Subjectivity_Score)\n",
    "    Average_Sentence_Length_r.append(Average_Sentence_Length)\n",
    "    Percentage_of_Complex_words_r.append(Percentage_of_Complex_words)\n",
    "    Fog_Index_r.append(Fog_Index)\n",
    "    Average_Number_of_Words_Per_Sentence_r.append(Average_Number_of_Words_Per_Sentence)\n",
    "    num_complex_words_r.append(num_complex_words)\n",
    "    total_num_of_words_r.append(total_num_of_words)\n",
    "    SYLLABLE_PER_WORD_r.append(SYLLABLE_PER_WORD)\n",
    "    num_personal_pronouns_r.append(num_personal_pronouns)\n",
    "    avg_word_length_r.append(avg_word_length)\n",
    "\n",
    "\n",
    "output = {\n",
    "    \"URL_ID\": id_r,\n",
    "    \"POSITIVE SCORE\": pos_score_r,\n",
    "    \"NEGATIVE SCORE\": neg_score_r,\n",
    "    \"POLARITY SCORE\": Polarity_Score_r,\n",
    "    \"SUBJECTIVITY SCORE\": Subjectivity_Score_r,\n",
    "    \"AVG SENTENCE LENGTH\": Average_Sentence_Length_r,\n",
    "    \"PERCENTAGE OF COMPLEX WORDS\": Percentage_of_Complex_words_r,\n",
    "    \"FOG INDEX\": Fog_Index_r,\n",
    "    \"AVG NUMBER OF WORDS PER SENTENCE\": Average_Number_of_Words_Per_Sentence_r,\n",
    "    \" COMPLEX WORD COUNT\": num_complex_words_r,\n",
    "    \"WORD COUNT \": total_num_of_words_r,\n",
    "    \"SYLLABLE PER WORD \": SYLLABLE_PER_WORD_r,\n",
    "    \"PERSONAL PRONOUNS\": num_personal_pronouns_r,\n",
    "    \"AVG WORD LENGTH\": avg_word_length_r,\n",
    "}\n",
    "\n",
    "output_df = pd.DataFrame(output).set_index(\"URL_ID\")\n",
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "257e9704",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df.to_excel(\"Output Data.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6069ab91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
